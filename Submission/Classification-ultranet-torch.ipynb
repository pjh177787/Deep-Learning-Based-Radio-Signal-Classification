{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# from tensorflow.keras.layers import Input,Reshape,ZeroPadding2D,MaxPool2D,Conv1D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "# from tensorflow.keras import layers\n",
    "# import tensorflow.keras.models as Model\n",
    "# from tensorflow.keras.regularizers import *\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24960, 1024, 2)\n",
      "(24960,)\n",
      "(24960, 1)\n",
      "(6240, 1024, 2)\n",
      "(6240,)\n",
      "(6240, 1)\n",
      "../ExtractDataset/part1.h5\n",
      "../ExtractDataset/part2.h5\n",
      "../ExtractDataset/part3.h5\n",
      "../ExtractDataset/part4.h5\n",
      "../ExtractDataset/part5.h5\n",
      "../ExtractDataset/part6.h5\n",
      "../ExtractDataset/part7.h5\n",
      "../ExtractDataset/part8.h5\n",
      "../ExtractDataset/part9.h5\n",
      "../ExtractDataset/part10.h5\n",
      "../ExtractDataset/part11.h5\n"
     ]
    }
   ],
   "source": [
    "raw_data = h5py.File('../ExtractDataset/part0.h5')\n",
    "sample_size = raw_data['X'].shape[0]\n",
    "sample_split = int(sample_size*0.8)\n",
    "idx = np.random.permutation(sample_size)\n",
    "train_idx, test_idx = idx[:sample_split], idx[sample_split:]\n",
    "X_train = np.array(raw_data['X'][:][train_idx])\n",
    "Y_train = np.argmax(np.array(raw_data['Y'][:][train_idx]), axis=1)\n",
    "Z_train = np.array(raw_data['Z'][:][train_idx])\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(Z_train.shape)\n",
    "X_test = np.array(raw_data['X'][:][test_idx])\n",
    "Y_test = np.argmax(np.array(raw_data['Y'][:][test_idx]), axis=1)\n",
    "Z_test = np.array(raw_data['Z'][:][test_idx])\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(Z_test.shape)\n",
    "raw_data.close()\n",
    "\n",
    "for i in range(1,12):\n",
    "    filename = '../ExtractDataset/part'+str(i) + '.h5'\n",
    "    print(filename)\n",
    "    raw_data = h5py.File(filename,'r')\n",
    "    sample_size = raw_data['X'].shape[0]\n",
    "    sample_split = int(sample_size*0.8)\n",
    "    idx = np.random.permutation(sample_size)\n",
    "    train_idx, test_idx = idx[:sample_split], idx[sample_split:]\n",
    "    X_train = np.vstack((X_train, np.array(raw_data['X'][:][train_idx])))\n",
    "    Y_train = np.hstack((Y_train, np.argmax(np.array(raw_data['Y'][:][train_idx]), axis=1)))\n",
    "    Z_train = np.vstack((Z_train, np.array(raw_data['Z'][:][train_idx])))\n",
    "    X_test = np.vstack((X_test, np.array(raw_data['X'][:][test_idx])))\n",
    "    Y_test = np.hstack((Y_test, np.argmax(np.array(raw_data['Y'][:][test_idx]), axis=1)))\n",
    "    Z_test = np.vstack((Z_test, np.array(raw_data['Z'][:][test_idx])))\n",
    "    raw_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train-size： (299520, 1, 1024, 2)\n",
      "Y_train-size： (299520,)\n",
      "Z_train-size： (299520, 1)\n",
      "X_test-size： (74880, 1, 1024, 2)\n",
      "Y_test-size： (74880,)\n",
      "Z_test-size： (74880, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 1, 1024, 2))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, 1024, 2))\n",
    "print('X_train-size：',X_train.shape)\n",
    "print('Y_train-size：',Y_train.shape)\n",
    "print('Z_train-size：',Z_train.shape)\n",
    "print('X_test-size：',X_test.shape)\n",
    "print('Y_test-size：',Y_test.shape)\n",
    "print('Z_test-size：',Z_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "training_set = TensorDataset(torch.Tensor(X_train), torch.Tensor(Y_train))\n",
    "train_dataloader = DataLoader(training_set, batch_size=16, shuffle=True)\n",
    "testing_set = TensorDataset(torch.Tensor(X_test), torch.Tensor(Y_test))\n",
    "test_dataloader = DataLoader(testing_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "classes = ['32PSK',\n",
    " '16APSK',\n",
    " '32QAM',\n",
    " 'FM',\n",
    " 'GMSK',\n",
    " '32APSK',\n",
    " 'OQPSK',\n",
    " '8ASK',\n",
    " 'BPSK',\n",
    " '8PSK',\n",
    " 'AM-SSB-SC',\n",
    " '4ASK',\n",
    " '16PSK',\n",
    " '64APSK',\n",
    " '128QAM',\n",
    " '128APSK',\n",
    " 'AM-DSB-SC',\n",
    " 'AM-SSB-WC',\n",
    " '64QAM',\n",
    " 'QPSK',\n",
    " '256QAM',\n",
    " 'AM-DSB-WC',\n",
    " 'OOK',\n",
    " '16QAM']\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 16, kernel_size=(3, 2), padding='valid')\n",
    "        self.bn0 = nn.BatchNorm2d(16)\n",
    "        self.conv1 = nn.Conv2d(16, 32, kernel_size=(3, 1), padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 1), padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(3, 1), padding='same')\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=(3, 1), padding='same')\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=(3, 1), padding='same')\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=(3, 1), padding='same')\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.conv7 = nn.Conv2d(64, 32, kernel_size=(3, 1), padding='same')\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(4064, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x, x.size()[3])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aperture/anaconda3/envs/radioml/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] loss: 45369.346259\n",
      "Accuracy of the network: 28.203459 %\n"
     ]
    }
   ],
   "source": [
    "global_loss = []\n",
    "global_accu = []\n",
    "num_epochs = 50\n",
    "import time\n",
    "\n",
    "ts = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.type(torch.long)\n",
    "#         print(inputs, labels, outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print(inputs, labels, predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    global_loss.append(running_loss)\n",
    "    global_accu.append(100 * correct / total)\n",
    "    print('[Epoch %d] loss: %f' %\n",
    "          (epoch + 1, running_loss))\n",
    "    print('Accuracy of the network: %f %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "tt = time.time()\n",
    "print('Finished Training, elapsed %.3f seconds'%(tt-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2,figsize=(20,6))\n",
    "\n",
    "ax1.plot(history.history['accuracy'],'b', history.history['val_accuracy'], 'r')\n",
    "ax1.set_ylabel('Accuracy Rate',fontsize=12)\n",
    "ax1.set_xlabel('Iteration',fontsize=12)\n",
    "ax1.set_title('Categorical Cross Entropy ',fontsize=14)\n",
    "ax1.legend(['Training Accuracy','Validation Accuracy'],fontsize=12,loc='best')\n",
    "\n",
    "ax2.plot(history.history['loss'], 'b',history.history['val_loss'],'r')\n",
    "ax2.set_ylabel('Loss',fontsize=12)\n",
    "ax2.set_xlabel('Iteration',fontsize=12)\n",
    "ax2.set_title('Learning Curve ',fontsize=14)\n",
    "ax2.legend(['Training Loss','Validation Loss'],fontsize=12,loc='best')\n",
    "\n",
    "# plt.savefig('crosse_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "# Plot confusion matrix\n",
    "batch_size = 1024\n",
    "test_Y_hat = model.predict(X_test, batch_size=3000)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for i in range(0,X_test.shape[0]):\n",
    "    j = list(Y_test[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "plot_confusion_matrix(confnorm, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(confnorm)):\n",
    "    print(classes[i],confnorm[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "acc={}\n",
    "Z_test = Z[test_idx]\n",
    "Z_test = Z_test.reshape((len(Z_test)))\n",
    "SNRs = np.unique(Z_test)\n",
    "for snr in SNRs:\n",
    "    X_test_snr = X_test[Z_test==snr]\n",
    "    Y_test_snr = Y_test[Z_test==snr]\n",
    "    \n",
    "    pre_Y_test = model.predict(X_test_snr)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,X_test_snr.shape[0]):    #该信噪比下测试数据量\n",
    "        j = list(Y_test_snr[i,:]).index(1)   #正确类别下标\n",
    "        k = int(np.argmax(pre_Y_test[i,:])) #预测类别下标\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "   \n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print (\"Overall Accuracy %s: \"%snr, cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc.keys(),acc.values())\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('SNR')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
